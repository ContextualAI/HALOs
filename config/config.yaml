# random seed
seed: 1

# name for this experiment in the local run directory and on wandb
exp_name: ???

# datasets should be specified as a list (e.g., ++datasets=[shp,hh])
train_datasets: ???
test_datasets: ???

# debug mode (disables wandb, model checkpointing, etc.)
debug: false

# wandb configuration
wandb:
  enabled: true
  entity: null
  project: "halos"

# where trained models will be saved
# use HF environmental variables to control where the datasets, pretrained Huggingface models, etc. are saved 
# relevant HF env variables are HF_HOME
cache_dir: .cache/

local_run_dir: ${cache_dir}/${exp_name}

# whether to eval at the very beginning of training
do_first_eval: true

# prevent wandb from logging more than once per minimum_log_interval_secs
minimum_log_interval_secs: 1.0

# if true, save a checkpoint every eval_every steps, which can be set further below
intermediate_checkpoints: true

defaults:
- _self_
- model: ???
- loss: ???

# the trainer class to use (e.g. BasicTrainer, FSDPTrainer, TensorParallelTrainer); should be specfied by the loss config
trainer: BasicTrainer

template_tokens: []


## TRAINING SETTINGS

# the learning rate
lr: 5e-6

# the number of epochs to train for; if null, must specify n_examples
n_epochs: 1

# the number of examples to train for; if null, must specify n_epochs
n_examples: null

# the number of examples to evaluate on (leave as null to evaluate on all of them)
n_eval_examples: 256

# evaluate and save model every eval_every steps
eval_every: 1024

# save model every save_every steps (in case it's different from eval_every)
save_every: 5120

# whether to only step scheduler once across all processes; see  https://github.com/huggingface/accelerate/issues/2142
step_scheduler_with_optimizer: false

# the optimizer to use; should be one of the options listed here: https://pytorch.org/docs/stable/optim.html
optimizer: AdamW

# the weight decay for the optimizer
weight_decay: 0.01

# beta1 for AdamW
beta1: 0.9

# beta2 for AdamW
beta2: 0.999

# epsilon for AdamW
eps: 1e-5

# fraction of training spent in warmup
warmup: 0.10

# cache log probabilities of reference model
cache_reference_logprobs: false

# path to pickle file of previously cached log probabilities of reference model
load_reference_logprobs: null

# humanline alignment (from prospect theory)
humanline: false
humanline_gamma_R: 0.25
humanline_gamma_P: 0.75
humanline_beta_R: 1.0
humanline_beta_P: 1.0

# sync reference with policy every X examples (set to 1 for fully online alignment)
online: false

sync_every: null


## DATALOADER SETTINGS

# what fraction of undesirable data should be kept
# e.g., if this is 0.8, then a random 20% of the undesirable examples (x, y_undesirable) should be thrown away
# this is to study the effect of an imbalanced dataset while only working with data that comes in paired preference form
frac_unique_desirable: 1.0

# what fraction of desirable data should be kept
# this is to study the effect of an imbalanced dataset while only working with data that comes in paired preference form
frac_unique_undesirable: 1.0