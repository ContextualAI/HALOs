# Direct Preference Optimization
name: dpo

# the temperature parameter for DPO; lower values mean we care less about the reference model
beta: 0.1

trainer: DPOTrainer

dataloader: PairedPreferenceDataLoader

use_reference_model: true

hf_model_class: AutoModelForCausalLM

policy_hf_model_class: AutoModelForCausalLM

reference_hf_model_class: AutoModelForCausalLM