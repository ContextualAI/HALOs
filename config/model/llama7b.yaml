defaults:
  - base_model

name_or_path: huggyllama/llama-7b
block_name: LlamaDecoderLayer
# use_flash_attention: true