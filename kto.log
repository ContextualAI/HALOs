2025-01-30 20:03:11,701 - hydra.core.utils - DEBUG - Setting JobRuntime:name=UNKNOWN_NAME
2025-01-30 20:03:11,701 - hydra.core.utils - DEBUG - Setting JobRuntime:name=UNKNOWN_NAME
2025-01-30 20:03:11,701 - hydra.core.utils - DEBUG - Setting JobRuntime:name=UNKNOWN_NAME
2025-01-30 20:03:11,703 - hydra.core.utils - DEBUG - Setting JobRuntime:name=launch
2025-01-30 20:03:11,703 - hydra.core.utils - DEBUG - Setting JobRuntime:name=launch
2025-01-30 20:03:11,703 - hydra.core.utils - DEBUG - Setting JobRuntime:name=launch
2025-01-30 20:03:11,705 - hydra.core.utils - DEBUG - Setting JobRuntime:name=UNKNOWN_NAME
2025-01-30 20:03:11,705 - hydra.core.utils - DEBUG - Setting JobRuntime:name=launch
[2025-01-30 20:03:12,372][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
seed: 1
exp_name: llama3-8B-instruct-kto-prelec
datasets:
- ultrafeedback_armorm
debug: false
wandb:
  enabled: false
  entity: null
  project: halos
cache_dir: /scratch/gpfs/sl2998/models
local_run_dir: /scratch/gpfs/sl2998/models/llama3-8B-instruct-kto-prelec
do_first_eval: true
minimum_log_interval_secs: 1.0
intermediate_checkpoints: false
trainer: BasicTrainer
lr: 2.5e-06
n_epochs: 1
n_examples: 10000
n_eval_examples: 256
eval_every: 1024
optimizer: AdamW
warmup_steps: 150
cache_reference_logprobs: false
load_reference_logprobs: null
humanline: true
humanline_alpha: 1.5
frac_unique_desirable: 1.0
frac_unique_undesirable: 1.0
model:
  name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
  tokenizer_name_or_path: null
  load_from: null
  from_checkpoint: null
  block_name: LlamaDecoderLayer
  policy_dtype: bfloat16
  policy_online: true
  policy_online_update_steps: 32
  reference_dtype: bfloat16
  max_grad_norm: 5.0
  v_head_max_grad_norm: 0.1
  max_length: 1024
  max_prompt_length: 512
  activation_checkpointing: true
  batch_size: 32
  microbatch_size: 8.0
  gradient_accumulation_steps: 1
  eval_batch_size: 32
  eval_microbatch_size: 8.0
  attn_implementation: flash_attention_2
  use_peft: false
  load_lora_from: null
  peft:
    lora_r: 64
    lora_alpha: 256
    lora_dropout: 0.05
    target_modules: all-linear
  reward_model:
    path: /scratch/gpfs/sl2998/models/llama3-8B-reward/FINAL
    model_class: AutoModelForBradleyTerry
    dtype: float32
    attn_implementation: flash_attention_2
loss:
  beta: 0.25
  trainer: KTOTrainer
  dataloader: UnpairedPreferenceDataLoader
  desirable_weight: 1.0
  undesirable_weight: 1.0
humanline_M: 1.6
humanline_gamma: 1.01
sampling_datasets:
- alpacaeval
num_samples_per_prompt: 2
sampling_mode: train
feedback_type: pairwise

Making experiment directory /scratch/gpfs/sl2998/models/llama3-8B-instruct-kto-prelec
================================================================================
Writing to /scratch/gpfs/sl2998/models/llama3-8B-instruct-kto-prelec
================================================================================
Loading tokenizer meta-llama/Meta-Llama-3-8B-Instruct
Loading data
Loading ultrafeedback_armorm dataset (train split) from Huggingface...
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:12,710][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:12,711][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:12,757][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:12,757][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:12,758][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:12,758][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:12,758][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:12,759][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Processing ultrafeedback armorm:   0%|          | 0/59876 [00:00<?, ?it/s]Processing ultrafeedback armorm:   1%|          | 741/59876 [00:00<00:07, 7404.36it/s]Processing ultrafeedback armorm:   2%|▏         | 1482/59876 [00:00<00:08, 7242.36it/s]Processing ultrafeedback armorm:   4%|▎         | 2207/59876 [00:00<00:08, 7046.29it/s]Processing ultrafeedback armorm:   5%|▍         | 2913/59876 [00:00<00:08, 7050.97it/s]Processing ultrafeedback armorm:   6%|▌         | 3619/59876 [00:00<00:07, 7036.98it/s]Processing ultrafeedback armorm:   7%|▋         | 4356/59876 [00:00<00:07, 7094.61it/s]Processing ultrafeedback armorm:   8%|▊         | 5066/59876 [00:00<00:07, 7095.84it/s]Processing ultrafeedback armorm:  10%|▉         | 5810/59876 [00:00<00:07, 7203.96it/s]Processing ultrafeedback armorm:  11%|█         | 6531/59876 [00:00<00:07, 7178.70it/s]Processing ultrafeedback armorm:  12%|█▏        | 7249/59876 [00:01<00:07, 7042.37it/s]Processing ultrafeedback armorm:  13%|█▎        | 7984/59876 [00:01<00:07, 7134.64it/s]Processing ultrafeedback armorm:  15%|█▍        | 8745/59876 [00:01<00:07, 7273.95it/s]Processing ultrafeedback armorm:  16%|█▌        | 9473/59876 [00:01<00:07, 6908.07it/s]Processing ultrafeedback armorm:  17%|█▋        | 10217/59876 [00:01<00:07, 7059.50it/s]Processing ultrafeedback armorm:  18%|█▊        | 10968/59876 [00:01<00:06, 7189.95it/s]Processing ultrafeedback armorm:  20%|█▉        | 11690/59876 [00:01<00:09, 4968.73it/s]Processing ultrafeedback armorm:  21%|██        | 12403/59876 [00:01<00:08, 5454.73it/s]Processing ultrafeedback armorm:  22%|██▏       | 13113/59876 [00:02<00:11, 3998.66it/s]Processing ultrafeedback armorm:  23%|██▎       | 13883/59876 [00:02<00:09, 4707.54it/s]Processing ultrafeedback armorm:  24%|██▍       | 14626/59876 [00:02<00:08, 5291.25it/s]Processing ultrafeedback armorm:  26%|██▌       | 15348/59876 [00:02<00:07, 5744.27it/s]Processing ultrafeedback armorm:  27%|██▋       | 16071/59876 [00:02<00:07, 6116.66it/s]Processing ultrafeedback armorm:  28%|██▊       | 16808/59876 [00:02<00:06, 6445.60it/s]Processing ultrafeedback armorm:  29%|██▉       | 17549/59876 [00:02<00:06, 6707.05it/s]Processing ultrafeedback armorm:  30%|███       | 18259/59876 [00:02<00:07, 5821.06it/s]Processing ultrafeedback armorm:  32%|███▏      | 19001/59876 [00:03<00:06, 6211.50it/s]Processing ultrafeedback armorm:  33%|███▎      | 19759/59876 [00:03<00:06, 6574.58it/s]Processing ultrafeedback armorm:  34%|███▍      | 20496/59876 [00:03<00:05, 6793.90it/s]Processing ultrafeedback armorm:  35%|███▌      | 21200/59876 [00:03<00:06, 6418.26it/s]Processing ultrafeedback armorm:  37%|███▋      | 21928/59876 [00:03<00:05, 6654.41it/s]Processing ultrafeedback armorm:  38%|███▊      | 22687/59876 [00:03<00:05, 6916.85it/s]Processing ultrafeedback armorm:  39%|███▉      | 23400/59876 [00:03<00:05, 6977.45it/s]Processing ultrafeedback armorm:  40%|████      | 24112/59876 [00:03<00:05, 7017.65it/s]Processing ultrafeedback armorm:  41%|████▏     | 24846/59876 [00:03<00:04, 7109.26it/s]Processing ultrafeedback armorm:  43%|████▎     | 25570/59876 [00:03<00:04, 7145.85it/s]Processing ultrafeedback armorm:  44%|████▍     | 26325/59876 [00:04<00:04, 7264.16it/s]Processing ultrafeedback armorm:  45%|████▌     | 27055/59876 [00:04<00:04, 7255.60it/s]Processing ultrafeedback armorm:  46%|████▋     | 27790/59876 [00:04<00:04, 7282.52it/s]Processing ultrafeedback armorm:  48%|████▊     | 28531/59876 [00:04<00:04, 7318.54it/s]Processing ultrafeedback armorm:  49%|████▉     | 29264/59876 [00:04<00:04, 7222.08it/s]Processing ultrafeedback armorm:  50%|█████     | 30014/59876 [00:04<00:04, 7302.02it/s]Processing ultrafeedback armorm:  51%|█████▏    | 30745/59876 [00:04<00:03, 7299.80it/s]Processing ultrafeedback armorm:  53%|█████▎    | 31476/59876 [00:04<00:06, 4384.03it/s]Processing ultrafeedback armorm:  54%|█████▍    | 32290/59876 [00:05<00:05, 5147.73it/s]Processing ultrafeedback armorm:  55%|█████▌    | 32940/59876 [00:05<00:05, 4649.27it/s]Processing ultrafeedback armorm:  56%|█████▌    | 33671/59876 [00:05<00:05, 5220.17it/s]Processing ultrafeedback armorm:  57%|█████▋    | 34286/59876 [00:05<00:05, 4768.83it/s]Processing ultrafeedback armorm:  58%|█████▊    | 34939/59876 [00:05<00:04, 5069.17it/s]Processing ultrafeedback armorm:  59%|█████▉    | 35502/59876 [00:05<00:04, 5188.72it/s]Processing ultrafeedback armorm:  60%|██████    | 36200/59876 [00:05<00:04, 5645.90it/s]Processing ultrafeedback armorm:  62%|██████▏   | 36931/59876 [00:05<00:03, 6091.25it/s]Processing ultrafeedback armorm:  63%|██████▎   | 37660/59876 [00:06<00:03, 6420.91it/s]Processing ultrafeedback armorm:  64%|██████▍   | 38404/59876 [00:06<00:03, 6708.54it/s]Processing ultrafeedback armorm:  65%|██████▌   | 39127/59876 [00:06<00:03, 6854.53it/s]Processing ultrafeedback armorm:  67%|██████▋   | 39884/59876 [00:06<00:02, 7062.01it/s]Processing ultrafeedback armorm:  68%|██████▊   | 40624/59876 [00:06<00:02, 7157.73it/s]Processing ultrafeedback armorm:  69%|██████▉   | 41348/59876 [00:06<00:02, 6246.19it/s]Processing ultrafeedback armorm:  70%|███████   | 41999/59876 [00:06<00:03, 5543.92it/s]Processing ultrafeedback armorm:  71%|███████▏  | 42719/59876 [00:06<00:02, 5960.29it/s]Processing ultrafeedback armorm:  72%|███████▏  | 43400/59876 [00:06<00:02, 6183.98it/s]Processing ultrafeedback armorm:  74%|███████▎  | 44111/59876 [00:07<00:02, 6437.12it/s]Processing ultrafeedback armorm:  75%|███████▍  | 44840/59876 [00:07<00:02, 6676.22it/s]Processing ultrafeedback armorm:  76%|███████▌  | 45523/59876 [00:07<00:02, 5364.12it/s]Processing ultrafeedback armorm:  77%|███████▋  | 46110/59876 [00:07<00:02, 5173.54it/s]Processing ultrafeedback armorm:  78%|███████▊  | 46878/59876 [00:07<00:02, 5797.20it/s]Processing ultrafeedback armorm:  80%|███████▉  | 47617/59876 [00:07<00:01, 6213.97it/s]Processing ultrafeedback armorm:  81%|████████  | 48351/59876 [00:07<00:01, 6520.86it/s]Processing ultrafeedback armorm:  82%|████████▏ | 49029/59876 [00:07<00:01, 6313.90it/s]Processing ultrafeedback armorm:  83%|████████▎ | 49771/59876 [00:07<00:01, 6618.13it/s]Processing ultrafeedback armorm:  84%|████████▍ | 50449/59876 [00:08<00:01, 5667.26it/s]Processing ultrafeedback armorm:  85%|████████▌ | 51161/59876 [00:08<00:01, 6038.22it/s]Processing ultrafeedback armorm:  87%|████████▋ | 51885/59876 [00:08<00:01, 6360.01it/s]Processing ultrafeedback armorm:  88%|████████▊ | 52575/59876 [00:08<00:01, 6507.60it/s]Processing ultrafeedback armorm:  89%|████████▉ | 53245/59876 [00:08<00:01, 3669.33it/s]Processing ultrafeedback armorm:  90%|█████████ | 53939/59876 [00:08<00:01, 4262.84it/s]Processing ultrafeedback armorm:  91%|█████████▏| 54662/59876 [00:09<00:01, 4880.59it/s]Processing ultrafeedback armorm:  92%|█████████▏| 55380/59876 [00:09<00:00, 5405.43it/s]Processing ultrafeedback armorm:  94%|█████████▎| 56048/59876 [00:09<00:00, 5719.56it/s]Processing ultrafeedback armorm:  95%|█████████▍| 56765/59876 [00:09<00:00, 6096.74it/s]Processing ultrafeedback armorm:  96%|█████████▌| 57480/59876 [00:09<00:00, 6381.08it/s]Processing ultrafeedback armorm:  97%|█████████▋| 58222/59876 [00:09<00:00, 6669.99it/s]Processing ultrafeedback armorm:  98%|█████████▊| 58939/59876 [00:09<00:00, 6808.78it/s]Processing ultrafeedback armorm: 100%|█████████▉| 59670/59876 [00:09<00:00, 6951.13it/s]Processing ultrafeedback armorm: 100%|██████████| 59876/59876 [00:09<00:00, 6142.08it/s]
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:22,562][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:22,562][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Loading ultrafeedback_armorm dataset (test split) from Huggingface...
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:22,564][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:22,565][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:22,571][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-01-30 20:03:22,572][datasets.load][WARNING] - Using the latest cached version of the dataset since princeton-nlp/llama3-ultrafeedback-armorm couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:22,572][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
[2025-01-30 20:03:22,572][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'default' at /scratch/gpfs/sl2998/cache/datasets/princeton-nlp___llama3-ultrafeedback-armorm/default/0.0.0/9d189bae5856a823f3708d2c2bc4dbb43c90eb11 (last modified on Sat Dec  7 11:30:40 2024).
Processing ultrafeedback armorm:   0%|          | 0/1961 [00:00<?, ?it/s]Processing ultrafeedback armorm:  37%|███▋      | 722/1961 [00:00<00:00, 7218.37it/s]Processing ultrafeedback armorm:  75%|███████▌  | 1479/1961 [00:00<00:00, 7422.26it/s]Processing ultrafeedback armorm: 100%|██████████| 1961/1961 [00:00<00:00, 7421.52it/s]
Loading reference model from meta-llama/Meta-Llama-3-8B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.95it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.79it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.80it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.74it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.87it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.85it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.87it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.83it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.91it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.91it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.91it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.89it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.97it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.95it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.99it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s]
Loading policy from meta-llama/Meta-Llama-3-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.14it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.10it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.04it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.13it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.12it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.11it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.10it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  3.12it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  3.13it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  3.13it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  3.12it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]
Creating optimizer and scheduler
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.12it/s]
Loading reward model from /scratch/gpfs/sl2998/models/llama3-8B-reward/FINAL
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:04,  2.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:04,  2.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:04,  2.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.97s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.97s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.58s/it]
[2025-01-30 20:04:11,241][train.trainers][INFO] - Using AdamW optimizer with learning rate 2.5e-06
[2025-01-30 20:04:11,241][train.trainers][INFO] - Using AdamW optimizer with learning rate 2.5e-06
[2025-01-30 20:04:11,242][train.trainers][INFO] - Using AdamW optimizer with learning rate 2.5e-06
[2025-01-30 20:04:11,338][train.trainers][INFO] - Using AdamW optimizer with learning rate 2.5e-06
humanline: true
Policy model parameter shapes:
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([262669312])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])Policy model parameter shapes:

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])Policy model parameter shapes:_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([262667264])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
Policy model parameter shapes:_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.norm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.lm_head.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
Reference model parameter shapes before syncing:_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([262669312])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])


_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])



_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])


_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])


_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])


_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])



_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])


_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])


_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.norm.weight: torch.Size([2048])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.lm_head.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])Reference model parameter shapes before syncing:
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([262667264])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.norm.weight: torch.Size([2048])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.lm_head.weight: torch.Size([262667264])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
Reference model parameter shapes before syncing:
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.norm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.lm_head.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])


_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.norm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.lm_head.weight: torch.Size([262669312])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])Reference model parameter shapes before syncing:

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])


_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])


_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])


_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.norm.weight: torch.Size([2048])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.norm.weight: torch.Size([2048])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.lm_head.weight: torch.Size([262667264])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.lm_head.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.norm.weight: torch.Size([0])
_fsdp_wrapped_module.lm_head.weight: torch.Size([262669312])
Reference model parameter shapes after syncing:Reference model parameter shapes after syncing:Reference model parameter shapes after syncing:


Reference model parameter shapes after syncing:
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([0])
_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([0])_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([262667264])

_fsdp_wrapped_module.model.embed_tokens.weight: torch.Size([262669312])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])


_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])



_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])


_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])


_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])


_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])


_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])



_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])


_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])



_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])



_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])


_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])


_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])


_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])


_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.24._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.25._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.26._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.27._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])


_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.28._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])


_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])


_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])
_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.29._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([50327552])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([4200448])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([16777216])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([4194304])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])



_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([16777216])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.norm.weight: torch.Size([2048])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([0])

_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([12584960])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])
_fsdp_wrapped_module.lm_head.weight: torch.Size([262667264])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([54519808])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.30._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([4096])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.q_proj.weight: torch.Size([0])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.norm.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.k_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.norm.weight: torch.Size([0])
_fsdp_wrapped_module.lm_head.weight: torch.Size([262669312])_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.v_proj.weight: torch.Size([0])

_fsdp_wrapped_module.lm_head.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.self_attn.o_proj.weight: torch.Size([0])
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.gate_proj.weight: torch.Size([46135296])
syncing reference is done._fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.up_proj.weight: torch.Size([8392704])

_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.mlp.down_proj.weight: torch.Size([0])
8
_fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.input_layernorm.weight: torch.Size([0])
completions is done._fsdp_wrapped_module.model.layers.31._fsdp_wrapped_module.post_attention_layernorm.weight: torch.Size([0])

_fsdp_wrapped_module.model.norm.weight: torch.Size([2048])
_fsdp_wrapped_module.lm_head.weight: torch.Size([0])
before batch:
after batch:
not fsdp
not fsdp
not fsdp
before batch:
after batch:
not fsdp
not fsdp
not fsdp
before batch:
after batch:
not fsdp
not fsdp
not fsdp
training started...
before batch:
after batch:
config.cache_reference_logprobs: False
not fsdp
not fsdp
not fsdp
Non-root FSDP instance's `_is_root` should not have been set yet or should have been set to `False`
Non-root FSDP instance's `_is_root` should not have been set yet or should have been set to `False`
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 316, in <module>
    hydra_main()
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 316, in <module>
    hydra_main()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 313, in hydra_main
    main(config)
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 313, in hydra_main
    main(config)
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 302, in main
    trainer.train()
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 302, in main
    trainer.train()
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 341, in train
    loss, metrics = self.get_batch_metrics(batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 341, in train
    loss, metrics = self.get_batch_metrics(batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 1007, in get_batch_metrics
    policy_chosen_logps, policy_rejected_logps, policy_KL_logps = self.forward(self.policy, batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 1007, in get_batch_metrics
    policy_chosen_logps, policy_rejected_logps, policy_KL_logps = self.forward(self.policy, batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 966, in forward
    KL_logits = model(
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 966, in forward
    KL_logits = model(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 848, in forward
    args, kwargs = _root_pre_forward(self, self, args, kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 848, in forward
    args, kwargs = _root_pre_forward(self, self, args, kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 517, in _root_pre_forward
    _lazy_init(state, module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 517, in _root_pre_forward
    _lazy_init(state, module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 140, in _lazy_init
    _share_state_and_init_handle_attrs(state, root_module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 140, in _lazy_init
    _share_state_and_init_handle_attrs(state, root_module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 209, in _share_state_and_init_handle_attrs
    _p_assert(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 209, in _share_state_and_init_handle_attrs
    _p_assert(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/utils.py", line 166, in _p_assert
    traceback.print_stack()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/utils.py", line 166, in _p_assert
    traceback.print_stack()
Non-root FSDP instance's `_is_root` should not have been set yet or should have been set to `False`
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 316, in <module>
    hydra_main()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 313, in hydra_main
    main(config)
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 302, in main
    trainer.train()
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 341, in train
    loss, metrics = self.get_batch_metrics(batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 1007, in get_batch_metrics
    policy_chosen_logps, policy_rejected_logps, policy_KL_logps = self.forward(self.policy, batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 966, in forward
    KL_logits = model(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 848, in forward
    args, kwargs = _root_pre_forward(self, self, args, kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 517, in _root_pre_forward
    _lazy_init(state, module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 140, in _lazy_init
    _share_state_and_init_handle_attrs(state, root_module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 209, in _share_state_and_init_handle_attrs
    _p_assert(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/utils.py", line 166, in _p_assert
    traceback.print_stack()
Non-root FSDP instance's `_is_root` should not have been set yet or should have been set to `False`
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 316, in <module>
    hydra_main()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 313, in hydra_main
    main(config)
  File "/scratch/gpfs/sl2998/workspace/HALOs/launch.py", line 302, in main
    trainer.train()
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 341, in train
    loss, metrics = self.get_batch_metrics(batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 1007, in get_batch_metrics
    policy_chosen_logps, policy_rejected_logps, policy_KL_logps = self.forward(self.policy, batch)
  File "/scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py", line 966, in forward
    KL_logits = model(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 848, in forward
    args, kwargs = _root_pre_forward(self, self, args, kwargs)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 517, in _root_pre_forward
    _lazy_init(state, module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 140, in _lazy_init
    _share_state_and_init_handle_attrs(state, root_module)
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 209, in _share_state_and_init_handle_attrs
    _p_assert(
  File "/home/sl2998/.conda/envs/halos/lib/python3.10/site-packages/torch/distributed/utils.py", line 166, in _p_assert
    traceback.print_stack()
get_batch_metrics is not okay
--Return--
--Return----Return--

> /scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py(1043)get_batch_metrics()->None
-> import pdb; pdb.set_trace()
--Return--
> /scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py(1043)get_batch_metrics()->None
-> import pdb; pdb.set_trace()
> /scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py(1043)get_batch_metrics()->None
-> import pdb; pdb.set_trace()
(Pdb) (Pdb) (Pdb) > /scratch/gpfs/sl2998/workspace/HALOs/train/trainers.py(1043)get_batch_metrics()->None
-> import pdb; pdb.set_trace()
(Pdb) FullyShardedDataParallel(
  (_fsdp_wrapped_module): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 4096)
      (layers): ModuleList(
        (0-31): 32 x FullyShardedDataParallel(
          (_fsdp_wrapped_module): LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((0,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((0,), eps=1e-05)
          )
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
  )
)
(Pdb) FullyShardedDataParallel(
  (_fsdp_wrapped_module): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 4096)
      (layers): ModuleList(
        (0-31): 32 x FullyShardedDataParallel(
          (_fsdp_wrapped_module): LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
      )
      (norm): LlamaRMSNorm((0,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
  )
)
(Pdb) 